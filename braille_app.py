# -*- coding: utf-8 -*-
"""Braille_app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1a5IXk2As_FlbSKVz_XBUagEmpLQjnw3R
"""

!pip install streamlit

!pip install gTTS

import streamlit as st
import numpy as np
import cv2
import torch
import torch.nn as nn
import torchvision.transforms as transforms
from PIL import Image
import joblib
from skimage.feature import hog
import gtts
from io import BytesIO

from google.colab import drive
drive.mount('/content/drive')

# Define CNNFeatureExtractor (same as notebook)
class CNNFeatureExtractor(nn.Module):
    def __init__(self, num_classes=26):
        super().__init__()
        self.features = nn.Sequential(
            nn.Conv2d(1, 16, 3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(4),
            nn.Conv2d(16, 32, 3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(4),
            nn.Conv2d(32, 64, 3, padding=1),
            nn.ReLU(),
            nn.AdaptiveAvgPool2d((4, 4))
        )
        self.classifier = nn.Sequential(
            nn.Linear(64*4*4, 128),
            nn.ReLU(),
            nn.Linear(128, num_classes)
        )

    def forward(self, x, extract_features=True):
        x = self.features(x)
        if extract_features:
            return x.flatten(start_dim=1)
        return self.classifier(x.flatten(start_dim=1))

# Define ViTFeatureExtractor (same as notebook)
class ViTFeatureExtractor(nn.Module):
    def __init__(self):
        super().__init__()
        self.vit = torch.hub.load('facebookresearch/deit:main',
                                 'deit_tiny_patch16_224', pretrained=True)
        self.vit.patch_embed.proj = nn.Conv2d(1, 192, kernel_size=16, stride=16)
        self.vit.head = nn.Identity()

    def forward(self, x):
        return self.vit(x)

# Load models
@st.cache_resource
def load_models():
    # Load feature extractors
    cnn = CNNFeatureExtractor()
    cnn.load_state_dict(torch.load('/content/drive/My Drive/Blessed capstone/cnn_model.pth', map_location='cpu'))
#    cnn.load_state_dict(torch.load('cnn_model.pth', map_location='cpu'))
    cnn.eval()

    vit = ViTFeatureExtractor()
    vit.load_state_dict(torch.load('/content/drive/My Drive/Blessed capstone/vit_model.pth', map_location='cpu'))
    vit.eval()

    # Load XGBoost classifier
    xgb_model = joblib.load('/content/drive/My Drive/Blessed capstone/xgboost_cnn_vit_hog_model.pkl')
   # xgb_model = joblib.load('xgboost_cnn_vit_hog_model.pkl')

    return cnn, vit, xgb_model

# Image transformation pipeline
transform = transforms.Compose([
    transforms.Grayscale(num_output_channels=1),
    transforms.Resize((32, 32)),
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))
])

vit_transform = transforms.Compose([
    transforms.Grayscale(num_output_channels=1),
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5])
])

# Feature extraction function
def extract_features(image):
    # HOG features
    img_np = np.array(image.convert('L'))
    hog_feat = hog(img_np, orientations=8, pixels_per_cell=(16,16),
                  cells_per_block=(1,1), visualize=False)

    # CNN features
    cnn_input = transform(image).unsqueeze(0)
    with torch.no_grad():
        cnn_feat = cnn(cnn_input).numpy().flatten()

    # ViT features
    vit_input = vit_transform(image).unsqueeze(0)
    with torch.no_grad():
        vit_feat = vit(vit_input).numpy().flatten()

    return np.concatenate([hog_feat, cnn_feat, vit_feat])

# Class mapping
braille_to_text = {
    0: 'a', 1: 'b', 2: 'c', 3: 'd', 4: 'e', 5: 'f', 6: 'g', 7: 'h',
    8: 'i', 9: 'j', 10: 'k', 11: 'l', 12: 'm', 13: 'n', 14: 'o',
    15: 'p', 16: 'q', 17: 'r', 18: 's', 19: 't', 20: 'u', 21: 'v',
    22: 'w', 23: 'x', 24: 'y', 25: 'z'
}

# Streamlit app
st.title("Braille Recognition System")
st.write("Upload an image of a Braille character for recognition")

uploaded_file = st.file_uploader("Choose an image...", type=["jpg", "jpeg", "png"])

# Load models
cnn, vit, xgb_model = load_models()

if uploaded_file is not None:
    # Display image
    image = Image.open(uploaded_file)
    st.image(image, caption='Uploaded Image', width=200)

    # Extract features
    features = extract_features(image)

    # Predict
    prediction = xgb_model.predict([features])[0]
    char = braille_to_text[prediction]

    st.success(f"Predicted Character: {char.upper()} ({char})")

    # Text-to-speech
    tts = gtts.gTTS(char)
    audio_bytes = BytesIO()
    tts.write_to_fp(audio_bytes)
    audio_bytes.seek(0)

    st.audio(audio_bytes, format='audio/mp3')
    st.download_button("Download Audio", audio_bytes, "braille_audio.mp3", "audio/mp3")